#Run upgrade

apt-get update
 
#Unzip/Build Essentials - GCC Compiler/Python Development/SASL/Pandas
apt-get install unzip build-essential python-dev libsasl2-dev python-pandas binutils

python -V  # should be 2.7x

vi /etc/bash.bashrc
export AIRFLOW_HOME=/airflow

mkdir /airflow
mkdir /airflow/dags
mkdir /airflow/logs
mkdir /airflow/plugins

# install pip
apt-get install python-pip
pip install --upgrade pip

pip install airflow

##other dependencies

pip install airflow[mysql] 
pip install airflow[celery] 
pip install airflow[rabbitmq]

cd /airflow
airflow initdb

## UPDATE custome config file

airflow webserver $* 
airflow scheduler


########################################################################
# Check syntax errors for your dag
python ~/airflow/dags/helloworld.py

# Print the list of active DAGs
airflow list_dags

# Print the list of tasks the "helloworld" dag_id
airflow list_tasks itcps_HelloWorld

# Print the hierarchy of tasks in the tutorial DAG
airflow list_tasks itcps_HelloWorld --tree

# Test your tasks in your dag
airflow test [DAG_ID] [TASK_ID] [EXECUTION_DATE]
airflow test itcps_HelloWorld sleep 2017-04-15

# Backfill: execute jobs that are not done in the past
airflow backfill itcps_HelloWorld -s 2017-04-15 -e 2017-04-17
#######################################################################


#Phase-2 - with mysql 

## Install MySQL on Ubuntu
apt-get install python-dev libmysqlclient-dev mysql-server mysql-client
pip install MySQL-python

mysql_secure_installation

mysql database	 : anixDb
mysql user 	    : airflowDbUser
mysql user pass	: Airflow!1PW

CREATE DATABASE anixDb CHARACTER SET utf8 COLLATE utf8_unicode_ci;
CREATE USER 'airflowDbUser'@'localhost' IDENTIFIED BY 'Airflow!1PW';
GRANT ALL PRIVILEGES ON * . * TO 'airflowDbUser'@'localhost';
FLUSH PRIVILEGES;

#password change if needed
ALTER USER 'airflowDbUser'@'localhost' IDENTIFIED BY 'Airflow!1PW';


# Change the executor to Local Executor and db
executor = LocalExecutor

sql_alchemy_conn = postgresql+psycopg2://your_postgres_user_name:your_postgres_password@host_name/database_name




######################################################################

# Start Web Server
nohup airflow webserver $* >> /airflow/logs/webserver.logs &

#for stop
for pid in $(ps -ef | grep -e "ww" -e airflow | awk '{print $2}'); do kill -9 $pid; done

#Start Celery Workers
nohup airflow worker $* >> /airflow/logs/celery.logs &

#Start Scheduler
nohup airflow scheduler >> /airflow/logs/scheduler.logs &

#Navigate to the Airflow UI
http://{HOSTNAME}:8080/admin/

##update mysql info in connection at admin menu

#Start Flower (Optional)
#Flower is a web UI built on top of Celery, to monitor your workers.

nohup airflow flower >> /airflow/logs/flower.logs &

#Navigate to the Flower UI (Optional)
http://{HOSTNAME}:5556/
